{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script async src=\"https://www.googletagmanager.com/gtag/js?id=UA-59152712-8\"></script>\n",
    "<script>\n",
    "  window.dataLayer = window.dataLayer || [];\n",
    "  function gtag(){dataLayer.push(arguments);}\n",
    "  gtag('js', new Date());\n",
    "\n",
    "  gtag('config', 'UA-59152712-8');\n",
    "</script>\n",
    "\n",
    "# Convert LaTeX Sentence to SymPy Expression\n",
    "\n",
    "## Author: Ken Sible\n",
    "\n",
    "## The following module will demonstrate a recursive descent parser for LaTeX.\n",
    "\n",
    "### NRPy+ Source Code for this module:\n",
    "1. [latex_parser.py](../edit/latex_parser.py); [\\[**tutorial**\\]](Tutorial-LaTeX_SymPy_Conversion.ipynb) The latex_parser.py script will convert a LaTeX sentence to a SymPy expression using the following function: parse(sentence)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc'></a>\n",
    "\n",
    "# Table of Contents\n",
    "$$\\label{toc}$$\n",
    "\n",
    "1. [Step 1](#intro): Introduction: Lexical Analysis and Syntax Analysis\n",
    "1. [Step 2](#sandbox): Demonstration and Sandbox (LaTeX Parser)\n",
    "1. [Step 3](#tensor): Tensor Support with Einstein Notation (WIP)\n",
    "1. [Step 4](#latex_pdf_output): $\\LaTeX$ PDF Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "# Step 1: Lexical Analysis and Syntax Analysis \\[Back to [top](#toc)\\]\n",
    "$$\\label{intro}$$\n",
    "\n",
    "In the following section, we discuss [lexical analysis](https://en.wikipedia.org/wiki/Lexical_analysis) (lexing) and [syntax analysis](https://en.wikipedia.org/wiki/Parsing) (parsing). In the process of lexical analysis, a lexer will tokenize a character string, called a sentence, using substring pattern matching (or tokenizing). We implemented a regex-based lexer for NRPy+, which does pattern matching using a [regular expression](https://en.wikipedia.org/wiki/Regular_expression) for each token pattern. In the process of syntax analysis, a parser will receive a token iterator from the lexer and build a parse tree containing all syntactic information of the language, as specified by a [formal grammar](https://en.wikipedia.org/wiki/Formal_grammar). We implemented a [recursive descent parser](https://en.wikipedia.org/wiki/Recursive_descent_parser) for NRPy+, which will build a parse tree in [preorder](https://en.wikipedia.org/wiki/Tree_traversal#Pre-order_(NLR)), starting from the root [nonterminal](https://en.wikipedia.org/wiki/Terminal_and_nonterminal_symbols), using a [right recursive](https://en.wikipedia.org/wiki/Left_recursion) grammar. The following right recursive, [context-free grammar](https://en.wikipedia.org/wiki/Context-free_grammar) was written for parsing [LaTeX](https://en.wikipedia.org/wiki/LaTeX), adhering to the canonical (extended) [BNF](https://en.wikipedia.org/wiki/Backus%E2%80%93Naur_form) notation used for describing a context-free grammar:\n",
    "```\n",
    "<ROOT>       -> <EXPRESSION> | ( <CONFIG> | <ASSIGNMENT> ) { <LINE_BREAK> ( <CONFIG> | <ASSIGNMENT> ) }*\n",
    "<CONFIG>     -> '%' <ARRAY> '[' <INTEGER> ']' [ ':' <SYMMETRY> ] { ',' <ARRAY> '[' <INTEGER> ']' [ ':' <SYMMETRY> ] }*\n",
    "<ASSIGNMENT> -> <VARIABLE> = <EXPRESSION>\n",
    "<EXPRESSION> -> <TERM> { ( '+' | '-' ) <TERM> }*\n",
    "<TERM>       -> <FACTOR> { [ '/' ] <FACTOR> }*\n",
    "<FACTOR>     -> <BASE> { '^' <EXPONENT> }*\n",
    "<BASE>       -> [ '-' ] ( <ATOM> | '(' <EXPRESSION> ')' | '[' <EXPRESSION> ']' )\n",
    "<EXPONENT>   -> <BASE> | '{' <BASE> '}'\n",
    "<ATOM>       -> <VARIABLE> | <NUMBER> | <COMMAND>\n",
    "<VARIABLE>   -> <ARRAY> | <SYMBOL> [ '_' ( <SYMBOL> | <INTEGER> ) ]\n",
    "<NUMBER>     -> <RATIONAL> | <DECIMAL> | <INTEGER>\n",
    "<COMMAND>    -> <SQRT> | <FRAC>\n",
    "<SQRT>       -> '\\\\sqrt' [ '[' <INTEGER> ']' ] '{' <EXPRESSION> '}'\n",
    "<FRAC>       -> '\\\\frac' '{' <EXPRESSION> '}' '{' <EXPRESSION> '}'\n",
    "<ARRAY>      -> ( <SYMBOL | <TENSOR> ) \n",
    "                    [ '_' ( <SYMBOL> | '{' { <SYMBOL> }+ '}' ) [ '^' ( <SYMBOL> | '{' { <SYMBOL> }+ '}' ) ]\n",
    "                    | '^' ( <SYMBOL> | '{' { <SYMBOL> }+ '}' ) [ '_' ( <SYMBOL> | '{' { <SYMBOL> }+ '}' ) ] ]\n",
    "```\n",
    "\n",
    "<small>**Source**: Robert W. Sebesta. Concepts of Programming Languages. Pearson Education Limited, 2016.</small>\n",
    "\n",
    "**TODO:** DEMONSTRATE PARSE TREE FOR A SIMPLE EXPRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from latex_parser import * # Import NRPy+ module for lexing and parsing LaTeX\n",
    "from sympy import srepr    # Import SymPy function for expression tree representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQRT_CMD, LEFT_BRACE, INTEGER, RIGHT_BRACE, LEFT_PAREN, SYMBOL, PLUS, RATIONAL, RIGHT_PAREN, CARET, INTEGER\n"
     ]
    }
   ],
   "source": [
    "lexer = Lexer(); lexer.initialize(r'\\sqrt{5}(x + 2/3)^2')\n",
    "print(', '.join(token for token in lexer.tokenize()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sqrt(5)*(x + 2/3)**2 : Mul(Pow(Integer(5), Rational(1, 2)), Pow(Add(Symbol('x'), Rational(2, 3)), Integer(2)))\n"
     ]
    }
   ],
   "source": [
    "expr = parse(r'\\sqrt{5}(x + 2/3)^2', expression=True)\n",
    "print(expr, ':', srepr(expr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sandbox'></a>\n",
    "\n",
    "# Step 2: Demonstration and Sandbox (LaTeX Parser) \\[Back to [top](#toc)\\]\n",
    "$$\\label{sandbox}$$\n",
    "\n",
    "We implemented a wrapper function for the parse() method that will accept a LaTeX sentence and return a SymPy expression. Furthermore, the entire parsing module was designed for extendibility. We apply the following procedure for extending parser functionality to include an unsupported LaTeX command: append that command to the grammar dictionary in the Lexer class with the mapping regex:token, write a grammar abstraction (similar to a regular expression) for that command, add the associated nonterminal (the command name) to the command abstraction in the Parser class, and finally implement the straightforward (private) method for parsing the grammar abstraction. We shall demonstrate the extension procedure using the `\\sqrt` LaTeX command.\n",
    "\n",
    "```<SQRT> -> '\\\\sqrt' [ '[' <INTEGER> ']' ] '{' <EXPRESSION> '}'```\n",
    "```\n",
    "def _sqrt(self):\n",
    "    if self.accept('LEFT_BRACKET'):\n",
    "        integer = self.lexer.lexeme\n",
    "        self.expect('INTEGER')\n",
    "        root = Rational(1, integer)\n",
    "        self.expect('RIGHT_BRACKET')\n",
    "    else: root = Rational(1, 2)\n",
    "    self.expect('LEFT_BRACE')\n",
    "    expr = self.__expr()\n",
    "    self.expect('RIGHT_BRACE')\n",
    "    return Pow(expr, root)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha_0**(1/3)\n"
     ]
    }
   ],
   "source": [
    "print(parse(r'\\sqrt[3]{\\alpha_0}', expression=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to expression parsing, we included support for equation parsing, which will produce a dictionary mapping LHS $\\mapsto$ RHS, where LHS must be a symbol, and insert that mapping into the global namespace of the previous stack frame, as demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2**(n/2)*n\n"
     ]
    }
   ],
   "source": [
    "parse(r'x = n\\sqrt{2}^n'); print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented robust error messaging, using the custom `ParseError` exception, which should handle every conceivable case to identify, as detailed as possible, invalid syntax inside of a LaTeX sentence. The following are runnable examples of possible error messages (simply uncomment and run the cell):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse(r'\\sqrt[*]{2}')\n",
    "    # ParseError: \\sqrt[*]{2}\n",
    "    #                   ^\n",
    "    # unexpected '*' at position 6\n",
    "\n",
    "# parse(r'\\sqrt[0.5]{2}')\n",
    "    # ParseError: \\sqrt[0.5]{2}\n",
    "    #                   ^\n",
    "    # expected token INTEGER at position 6\n",
    "\n",
    "# parse(r'\\command{}')\n",
    "    # ParseError: \\command{}\n",
    "    #             ^\n",
    "    # unsupported command '\\command' at position 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sandbox code cell below, you can experiment with the LaTeX parser using the wrapper function parse(sentence), where sentence must be a [raw string](https://docs.python.org/3/reference/lexical_analysis.html) to interpret a backslash as a literal character rather than an [escape sequence](https://en.wikipedia.org/wiki/Escape_sequence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Sandbox Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tensor'></a>\n",
    "\n",
    "# Step 3: Tensor Support with Einstein Notation (WIP) \\[Back to [top](#toc)\\]\n",
    "$$\\label{tensor}$$\n",
    "\n",
    "In the following section, we demonstrate the current parser support for tensor notation using the Einstein summation convention. The first example will parse a simple equation for raising an index using the metric tensor, while assuming a 3-dimensional space (i.e. `i` and `j` range over `0, 1, 2`) and $g_{ij}$ symmetric:\n",
    "$$v^i=g_{ij}v_j.$$\n",
    "The second example will parse an equation for a simple tensor contraction, while assuming $h^\\mu{}_\\mu$ not symmetric:\n",
    "$$h=h^\\mu{}_\\mu.$$\n",
    "\n",
    "**TODO:** REMOVE THE FOLLOWING PARAGRAPH AND REPLACE WITH CONFIGURATION PARAGRAPH\n",
    "\n",
    "We should mention that a future build of the parser would require a configuration file be specified before parsing a tensorial equation. The process demonstrated below for declaring a tensor, adding that tensor to a namespace, and passing that namespace to the parser would be eliminated.\n",
    "\n",
    "**Configuration Syntax**: `% <TENSOR> [<DIMENSION>]: <SYMMETRY>, <TENSOR> [<DIMENSION>]: <SYMMETRY>, ... ;`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gUU', 'vD', 'vU']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse(r\"\"\"\n",
    "    % g^{ij} [3]: sym01, v_j [3];\n",
    "    v^i = g^{ij}v_j\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gUU = [[gUU00, gUU01, gUU02], [gUU01, gUU11, gUU12], [gUU02, gUU12, gUU22]]\n",
      "vU  = [vD0, vD1, vD2]\n",
      "vD  = [gUU00*vD0 + gUU01*vD1 + gUU02*vD2, gUU01*vD0 + gUU11*vD1 + gUU12*vD2, gUU02*vD0 + gUU12*vD1 + gUU22*vD2]\n"
     ]
    }
   ],
   "source": [
    "print('gUU = %s\\nvU  = %s\\nvD  = %s' % (gUU, vD, vU))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h', 'hUD']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse(r\"\"\"\n",
    "    % h^\\mu_\\mu [4]: nosym;\n",
    "    h = h^\\mu{}_\\mu\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h   = hUD00 + hUD11 + hUD22 + hUD33\n",
      "hUD = [[hUD00, hUD01, hUD02, hUD03], [hUD10, hUD11, hUD12, hUD13], [hUD20, hUD21, hUD22, hUD23], [hUD30, hUD31, hUD32, hUD33]]\n"
     ]
    }
   ],
   "source": [
    "print('h   = %s\\nhUD = %s' % (h, hUD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: ADD SECTION ABOUT ERROR HANDLING FOR TENSOR INDEXING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='latex_pdf_output'></a>\n",
    "\n",
    "# Step 4: Output this notebook to $\\LaTeX$-formatted PDF file \\[Back to [top](#toc)\\]\n",
    "$$\\label{latex_pdf_output}$$\n",
    "\n",
    "The following code cell converts this Jupyter notebook into a proper, clickable $\\LaTeX$-formatted PDF file. After the cell is successfully run, the generated PDF may be found in the root NRPy+ tutorial directory, with filename\n",
    "[Tutorial-LaTeX_SymPy_Conversion.pdf](Tutorial-LaTeX_SymPy_Conversion.pdf) (Note that clicking on this link may not work; you may need to open the PDF file through another means.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Tutorial-LaTeX_SymPy_Conversion.tex, and compiled LaTeX file to PDF\n",
      "    file Tutorial-LaTeX_SymPy_Conversion.pdf\n"
     ]
    }
   ],
   "source": [
    "import cmdline_helper as cmd    # NRPy+: Multi-platform Python command-line interface\n",
    "cmd.output_Jupyter_notebook_to_LaTeXed_PDF(\"Tutorial-LaTeX_SymPy_Conversion\")"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
